{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c1389e3-71d7-48d7-b347-f8701ea0d418",
   "metadata": {},
   "source": [
    "# Generating .Wav Files from Splice Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428ef916-725e-4a37-925a-ceb3747d6449",
   "metadata": {},
   "source": [
    "The following models attempt to simplify my introduction into generative ai modeling. Earlier iterations of this assignment sought to use Google's Magenta to train a model using midi files/sequences to generate any sort of sound. I ran across multiple barriers that prevented me from pursuing this method (issues with packages and environments). This notebook is number 5 in a series of 7 using pytorch that attempt to train a generator to convert random noise into something less harsh and meaningless. The following model is my most successful attempt at getting this to work (it barely sounds like music, but atleast it's not random noise). The second model iterates on this method through implementing CNN architecture (although i was able to train, this was completely unsuccessful, and no improvement to this implementation was made in the notebooks that followed).\n",
    "\n",
    "The pre-write will outline the steps taken to build the first model (minus the frustration).\n",
    "\n",
    "1. Gather Training data -- .wav files were downloaded from Splice (a sample repo for music producers). Samples chosen were between 30 secs to a minute long. Instrumental only (no vox, minimal drums). About 220 samples across soul, gospel, rnb, and lofi hip hop categories were chosen. Primarily pianos, synths, and other melodic instruments.\n",
    "2. Pre-processing -- when downloaded these files were organized in various folder branches. A script to flatten the root folder/directory was used to write/organize our data in a more easily accessible fashion. Earlier modeling attempts at preprocessing and train models duplicate many of the files. New functions were used to remove duplicates. After duplicate files were verified and removed, we normalize the length and the sample rates of the .wav files.\n",
    "3. Loading the Data -- Each file was converted to tensors that could then be fed into our neural network using our 'data loader'.\n",
    "4. Neural Network and Hyper Parameter tuning -- This first model is a fully connected feedforward NN or Multi-layer perceptron. The parameters I changed primarily were batch size and number of epoch. Previous versions used smaller batches and more epochs. After the first 'successful' training to nearly an hour, we achieved something beyond just noise. I could tell i was moving in the correct direction when the noise has hints of something else (almost like a radio signal almost entirely obscured by static). By changing these parameters (as well as fixing my data directories), we were able to achieve results similar to the longer trainings (batch size 1 and 50 epochs) but much more quickly. I faced many issues with dimensionality which ultimately prevented me from continuing to tune my CNN version.\n",
    "5. The only form of validation done in this process was saving a generated .wav files and listening to it. The final result was a caucophonous/harmonically disonant drone with sparse elements of bell dings and rythmic textures. I never thought I would be so happy to hear something so awful sounding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fb988f0-0834-421f-b3af-0807d21b1fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied: flat_new/e17dec0980e9c5a98d55878cdf69a43f.wav\n",
      "Copied: flat_new/0c35034c25032f8af4ecd7fd05823c2a.wav\n",
      "Copied: flat_new/589de4c1ef2de26c172adc17a821a1b7.wav\n",
      "Copied: flat_new/5f9f29b959ddcaf4427942f445b4c32c.wav\n",
      "Copied: flat_new/12bc5bfcf99e5cfca7c9f5f3177c0aea.wav\n",
      "Copied: flat_new/fcdb17f8d4842fbbbfbc64e8d7e97020.wav\n",
      "Copied: flat_new/0dcbfd00524bec462bf274e1944fcf7a.wav\n",
      "Copied: flat_new/834d08b5e47de29f55946560f20d5fe0.wav\n",
      "Copied: flat_new/98287ff62cf2afa762d960164a2b9b82.wav\n",
      "Copied: flat_new/c8382806236dbb13b111cf54a1019648.wav\n",
      "Copied: flat_new/9d3fce79f095cf78e4eafd4ec6940124.wav\n",
      "Copied: flat_new/58d50f354073bff07733f24123e871ee.wav\n",
      "Copied: flat_new/036060341d0d564cb38f66e5df29960c.wav\n",
      "Copied: flat_new/cd536a01a601e6951e57f29c74745bd2.wav\n",
      "Copied: flat_new/802c402bc9727528d8571bb632491a83.wav\n",
      "Copied: flat_new/eea7d1afe34ef61161670cd0353b6357.wav\n",
      "Copied: flat_new/8506b824da08fd3a9ad9b73be6d6c070.wav\n",
      "Copied: flat_new/03e3fba6be54d43b2336cf584e94c3e1.wav\n",
      "Copied: flat_new/46ea08c8712708d1b2537bb8eb66a263.wav\n",
      "Copied: flat_new/ea1c20b29bc96035ebfdc3a1f740d927.wav\n",
      "Copied: flat_new/6e8c6b9d27a5a3c04ba390372f3b9959.wav\n",
      "Copied: flat_new/cb1badab148133e30b873835c32eab94.wav\n",
      "Copied: flat_new/4fdfb0c8980043828fd176c3d197145f.wav\n",
      "Copied: flat_new/e3b3557c5eab2681b7b2c77375a2c2ae.wav\n",
      "Copied: flat_new/991572cd58919f6c1227d204f831b144.wav\n",
      "Copied: flat_new/d035133ca28bf8518b6f21934b2c616d.wav\n",
      "Copied: flat_new/dbe29c67b52b9e5c994ffb654b90654e.wav\n",
      "Copied: flat_new/286cb2cded154c10c45ed2e5d1ae52da.wav\n",
      "Copied: flat_new/f19d69c3a01c4da90537a279bc5976bc.wav\n",
      "Copied: flat_new/d7d0bcc29dbd4ca2c72828bfa315e44c.wav\n",
      "Copied: flat_new/0a5a0314792885da5e756d64eceb1206.wav\n",
      "Copied: flat_new/546c1b6265d607dda79a51e124380881.wav\n",
      "Copied: flat_new/d023111a69cb48748ef065222709a5f5.wav\n",
      "Copied: flat_new/d033748f6e5e4bf246f6b23a9a437dd0.wav\n",
      "Copied: flat_new/1073ea0a2180fd7e88742316f25b8a6c.wav\n",
      "Copied: flat_new/4496558b42c46f13bc7b8e214d2101d9.wav\n",
      "Copied: flat_new/32cda181e6122f86d902094cc029bceb.wav\n",
      "Copied: flat_new/be683ab80bdd1fdecf10826359249656.wav\n",
      "Copied: flat_new/93e40774754941effd7d925f7c245bda.wav\n",
      "Copied: flat_new/071282abcbcdecc42f27d99ec4c4222c.wav\n",
      "Copied: flat_new/9403bc5542b92b16c3fe55065620ad8f.wav\n",
      "Copied: flat_new/d1fd752d770762dee394c86d8d6db5f2.wav\n",
      "Copied: flat_new/61a8bf3f4f5caa78f74e109727ce0a44.wav\n",
      "Copied: flat_new/6dc0dbc3e1ca6fa16569376a6b599e58.wav\n",
      "Copied: flat_new/4ace42304250d4e45405d5bf4d4542b7.wav\n",
      "Copied: flat_new/439f6afb0f861b9fe3f14f1a4d1a39a3.wav\n",
      "Copied: flat_new/80db6dab31f3972fef4a800593f76dba.wav\n",
      "Copied: flat_new/316d15b03c62e2c3fbff70b105272345.wav\n",
      "Copied: flat_new/a63e9ef35876d9fb9e3977eaa24cb0b4.wav\n",
      "Copied: flat_new/2d5a2700c2992f9a25b31fe3f17d83a8.wav\n",
      "Copied: flat_new/7b9887eb3d467e2be35c738147956841.wav\n",
      "Copied: flat_new/b2f8f9c605857aec416e5c81eba36d8f.wav\n",
      "Copied: flat_new/1b47e33caaebe1608edfd235ea1c4fae.wav\n",
      "Copied: flat_new/effe2b5b873dfd3df28487e2767f1602.wav\n",
      "Copied: flat_new/27b383ef03dabd514b6600e83082f7ee.wav\n",
      "Copied: flat_new/42db581a56613cb21a7f8c8f9152d9d2.wav\n",
      "Copied: flat_new/6ae24476deaf091c4229e6ad1882d701.wav\n",
      "Copied: flat_new/0027b09e23ba34dcbc6f57a78e9f6203.wav\n",
      "Copied: flat_new/d02fe3d103a7d749979451f397061f77.wav\n",
      "Copied: flat_new/6ab0a2adb50ae0fcc203437ef9bc6254.wav\n",
      "Copied: flat_new/858f14e243891e0e123261d53e8eeb71.wav\n",
      "Copied: flat_new/b8608a3d4b8f14b07b1d6493f75e7882.wav\n",
      "Copied: flat_new/1ffe4c36899c860456536ec348c227ea.wav\n",
      "Copied: flat_new/6a7ca808592928a6f5522b366acb7f20.wav\n",
      "Copied: flat_new/a5c5de508a9400631a304eedc5e607bb.wav\n",
      "Copied: flat_new/e742e2c6a3dfc7fc236bfceee930032d.wav\n",
      "Copied: flat_new/8c3ba6bfcd5dc23bbba562735b465022.wav\n",
      "Copied: flat_new/fa0c816fcf8abb0a9f20d94472fb2632.wav\n",
      "Copied: flat_new/8b1c262527f940eb7c11ef40860840f3.wav\n",
      "Copied: flat_new/f7ac4d8c501bc1932668a9a61ee8b224.wav\n",
      "Copied: flat_new/f11154f4b113277ac9167e276b4ceeba.wav\n",
      "Copied: flat_new/0b22a324b98c53ae98ca854800ce7ab3.wav\n",
      "Copied: flat_new/bab3fcd902d617bc0337510ac9b4cf3a.wav\n",
      "Copied: flat_new/f1d29307ecb055f6f1c5522649329def.wav\n",
      "Copied: flat_new/adda4ae781ff967d1071618eab0f6356.wav\n",
      "Copied: flat_new/f731ddcf0a89afe2bd79073743bbf82c.wav\n",
      "Copied: flat_new/9ae4824225ceeb13d486fbaca6f83877.wav\n",
      "Copied: flat_new/df52c8e489235e467543a01f8eb4e2a4.wav\n",
      "Copied: flat_new/28e62999f27f0a19bd8372b282d92efd.wav\n",
      "Copied: flat_new/c4dffe0367756223da8bc1cf5c11c88e.wav\n",
      "Copied: flat_new/e7bc67a63fda414e5f3827b2dee19dda.wav\n",
      "Copied: flat_new/898c1aa91185a02c5ea7c109387f7933.wav\n",
      "Copied: flat_new/a50483ed288ef66eb7e40e0bc78a1fbb.wav\n",
      "Copied: flat_new/a260b4c5c3b7965aaa513dbb65812610.wav\n",
      "Copied: flat_new/670115b1056eff73ff34276c3872e31d.wav\n",
      "Copied: flat_new/3286d51c7faaeca7aeb169375f53256d.wav\n",
      "Copied: flat_new/9cc5dffd9cab80b07c6d9c0563962532.wav\n",
      "Copied: flat_new/aeddef887f0a9a382acc30530f91db05.wav\n",
      "Copied: flat_new/74fa49ca0cb45a22c4401167e441fdbc.wav\n",
      "Copied: flat_new/47756c239122c88c625a2e511759e265.wav\n",
      "Copied: flat_new/37d8e78a04588ce6661e856f2ee54d3b.wav\n",
      "Copied: flat_new/9003178b508505d8f6c4988504d4d392.wav\n",
      "Copied: flat_new/cc970ca80f9c57af862ded338eca0d3a.wav\n",
      "Copied: flat_new/5435204ef3fc3100a40087e69c78a87f.wav\n",
      "Copied: flat_new/99ac8ed30d71a53084acd2d2e541f18a.wav\n",
      "Copied: flat_new/1752271bbd7c4b26df254c3caed5f04f.wav\n",
      "Copied: flat_new/12ad8a214eb997c3caea8726e8dafb86.wav\n",
      "Copied: flat_new/48161330c4a324f156bc6b0bac3eabcd.wav\n",
      "Copied: flat_new/c91b54a69a231093d22477bb76c01376.wav\n",
      "Copied: flat_new/4b3faffeace9e18112c2a3cd29d21a0e.wav\n",
      "Copied: flat_new/9b96f0d20d6031ce755a5def93f66b75.wav\n",
      "Copied: flat_new/49d1643cae44efc4cfcd6c6d1aab0c49.wav\n",
      "Copied: flat_new/2059a5ccd185aa484aff04173f7a928c.wav\n",
      "Copied: flat_new/59639b5d7ed933a6102948da71c044e7.wav\n",
      "Copied: flat_new/364ecd666877b78cf206726b23005a64.wav\n",
      "Copied: flat_new/474214a7488019f09ea767799b70539b.wav\n",
      "Copied: flat_new/7e1ddd4da03d35a969eb335eb4ce07a7.wav\n",
      "Copied: flat_new/5428038f974ac32f4f244d1d26422c21.wav\n",
      "Copied: flat_new/b54878f9e0f04aa8da2d2cefa0c7342c.wav\n",
      "Copied: flat_new/31ec259f2a8b56fcc628cc4d68985b08.wav\n",
      "Copied: flat_new/5fbf3b895d90f2e0d86314ab49925418.wav\n",
      "Copied: flat_new/1ba9e04f166f99ae5ae6663b2f354d6f.wav\n",
      "Copied: flat_new/f7d2f84634f8207e18c6545344ff6936.wav\n",
      "Copied: flat_new/5d84ef4d0844a874df96e72637c8f5e2.wav\n",
      "Copied: flat_new/d1afd8feb64f0cc6c5e28266c89f4c9f.wav\n",
      "Copied: flat_new/acf87fc8cc9ef62a081b90475e6812a9.wav\n",
      "Copied: flat_new/6b3946d60b1f9789955cca651f780e02.wav\n",
      "Copied: flat_new/c50c14eedece1aade96fc9e2d2d5aa52.wav\n",
      "Copied: flat_new/f3bc860f6085ee66a44c46ffb2327556.wav\n",
      "Copied: flat_new/dc4b1b28f3fe3e8bc53344eea1c5a930.wav\n",
      "Copied: flat_new/0546670d6fa6d547ce62894badbb9bcf.wav\n",
      "Copied: flat_new/9edeac61d06b7b7061ef84f5df50d8ee.wav\n",
      "Copied: flat_new/0b4a5f6b6e43c9fd777652d9512c58f0.wav\n",
      "Copied: flat_new/c9c29e83595e44d64edf4364fe5ac980.wav\n",
      "Copied: flat_new/6b0d78e62ab03b832d67f45717c1a525.wav\n",
      "Copied: flat_new/bbd546a72e1363e0bb71991c951a6cda.wav\n",
      "Copied: flat_new/457133f18de74f39b2590a8ffda00362.wav\n",
      "Copied: flat_new/84bd94c2808a56a5e3ee9cf4ab8f3ade.wav\n",
      "Copied: flat_new/c414f6a93e641dd4b221e2ab7810cb10.wav\n",
      "Copied: flat_new/7be6c7648972ab6e743036e04eb18872.wav\n",
      "Copied: flat_new/e4fa5b8f3d257c90ba934068c6640508.wav\n",
      "Copied: flat_new/43e50e828484ecb79ecfc24b957285de.wav\n",
      "Copied: flat_new/362fc4e57141606eda194e15cab557da.wav\n",
      "Copied: flat_new/930646fb702713fbc8fdff902df53a02.wav\n",
      "Copied: flat_new/3116cb4bad344748f11fbd763601df43.wav\n",
      "Copied: flat_new/754036a4a09591d92d9880b901f459b7.wav\n",
      "Copied: flat_new/8c8796500361d7db1bba46849708080d.wav\n",
      "Copied: flat_new/320db5b9247d9b964353b0583e9c97ca.wav\n",
      "Copied: flat_new/beea0546b10e250aed077696583bfeba.wav\n",
      "Copied: flat_new/499728803cec3bc3d77445808148e012.wav\n",
      "Copied: flat_new/4ab04a5690748c54870e16e7cac546aa.wav\n",
      "Copied: flat_new/ca568e6c148285a1c56dfc244dc736f8.wav\n",
      "Copied: flat_new/4443eca119486a11f1fe62b37b406b0a.wav\n",
      "Copied: flat_new/12adb8338b8805d56c40778910bc08fb.wav\n",
      "Copied: flat_new/3adf1ce222f07d8099b3adc4733e9b21.wav\n",
      "Copied: flat_new/e350b428a18ad2929e0d244463195b7f.wav\n",
      "Copied: flat_new/7f9fd9b79a13c32ab82bfcbfd21b0ea2.wav\n",
      "Copied: flat_new/380d6c0626609971582424f52d40b774.wav\n",
      "Copied: flat_new/1083f7e0d57bdb99d547d7dc7df31da6.wav\n",
      "Copied: flat_new/7530787808fe44bc7d146d7d86183ed3.wav\n",
      "Copied: flat_new/3930d6f96ddcb44efbe06c80a258dbf0.wav\n",
      "Copied: flat_new/0a2d429bdca34bb6a12907390d1e4cb4.wav\n",
      "Copied: flat_new/b5f7dca16ffa979ab5d1d69e21e53754.wav\n",
      "Copied: flat_new/bbd5da7e773aed11a76de9034332f30d.wav\n",
      "Copied: flat_new/227df6953c9d561d595ea5be9065d366.wav\n",
      "Copied: flat_new/f9d3b9600fd0375f5218035b91d14c8b.wav\n",
      "Copied: flat_new/7694b248866bab74c00c3ec369dfd6f3.wav\n",
      "Copied: flat_new/add0834ca1cf0e683748edd7bbfff29c.wav\n",
      "Copied: flat_new/faeb6f2fc2e78ba84d6e11783ba4bac0.wav\n",
      "Copied: flat_new/683bf3977910e8001089409a76a1945f.wav\n",
      "Copied: flat_new/b7ad2ebd30db369897162e2ba779ef53.wav\n",
      "Copied: flat_new/de0ec7c39cfab1962b96b15f1702afcc.wav\n",
      "Copied: flat_new/98ecbdc1f4a7a7260f6e3fb50325da92.wav\n",
      "Copied: flat_new/2ec32ea25e5646e9f3fc466dfcac9d17.wav\n",
      "Copied: flat_new/da58db88779c9415c66b7751abe29aa9.wav\n",
      "Copied: flat_new/971d834ae6d9a982e6334c94dd784429.wav\n",
      "Copied: flat_new/3f2ad68dd21cf6b2891fd9b41b5258dd.wav\n",
      "Copied: flat_new/0549c0cc9e72723dde53bbb2c17d1687.wav\n",
      "Copied: flat_new/b3814373c52b2a8b9e0f12d07ebe12d2.wav\n",
      "Copied: flat_new/075f06474fdd93288f7082fc48121739.wav\n",
      "Copied: flat_new/3ad21a252e7456872d1ea919c5ea8144.wav\n",
      "Copied: flat_new/cd44faefde06f0d2b62d1a5b7cd8418d.wav\n",
      "Copied: flat_new/4a8537599846d3b11796412b39f729a5.wav\n",
      "Copied: flat_new/647a39517743903fa3eee312f1d4919b.wav\n",
      "Copied: flat_new/06ac33b18edb70ac6dfb12f9ae0e7663.wav\n",
      "Copied: flat_new/8c951e50ff80367da28c379cca0fb608.wav\n",
      "Copied: flat_new/fe080f560a10674fa91b7bfea39b98de.wav\n",
      "Copied: flat_new/a6bd8fa7c037d5b31dccdc47992c00cd.wav\n",
      "Copied: flat_new/ed7460064a836e598c2bbb21ac319508.wav\n",
      "Copied: flat_new/5350d56a6778fed00f210cf1d0057e06.wav\n",
      "Copied: flat_new/56283e3f284990c8a046ab59d4b9390f.wav\n",
      "Copied: flat_new/9b83c90724b357a799fa4bf5a54f088b.wav\n",
      "Copied: flat_new/8147e1b4ef4707613f84c2d0d1b453f1.wav\n",
      "Copied: flat_new/db34dc0df2bdf636563c101b1b68877a.wav\n",
      "Copied: flat_new/ab04057b807a4d8f17b1601922c56b47.wav\n",
      "Copied: flat_new/e2a5acb1e0d01b5241a6ad927e586f6c.wav\n",
      "Copied: flat_new/184d2bb3751840a6068fc4a07a1bd873.wav\n",
      "Copied: flat_new/561741d411b85c29ac39acee008ce547.wav\n",
      "Copied: flat_new/95dd45d3feb1e6a68ed0ce03b554ea19.wav\n",
      "Copied: flat_new/5bed5ec25e01a2f25c66e0e973daf48a.wav\n",
      "Copied: flat_new/f82ffaf1c15de79594c0d8d0f6ed820f.wav\n",
      "Copied: flat_new/e211e616230fd6d005887b24b2460a6f.wav\n",
      "Copied: flat_new/597a9a01cb018f9dae5cb902c8b94467.wav\n",
      "Copied: flat_new/861a14a8f1f91fba27a9f9b8d2e62278.wav\n",
      "Copied: flat_new/9373fada51f59dec5f4ed14cb206fa32.wav\n",
      "Copied: flat_new/eabb5d9332defebbe657dff6a31e7ac8.wav\n",
      "Copied: flat_new/bf4c44a56610b0671c2c42ab869d5f08.wav\n",
      "Copied: flat_new/09393f8f35bfd326a2db5a72ad05cebb.wav\n",
      "Copied: flat_new/a75c7e601a16e78dd548db0025db5a97.wav\n",
      "Copied: flat_new/a1884ec84e59e218a399893b6e4252be.wav\n",
      "Copied: flat_new/5f2fc6969459ea60eef25f915d083c9c.wav\n",
      "Copied: flat_new/e297a9a03874925ea54bad68b8043871.wav\n",
      "Copied: flat_new/b4dafb0b82132d2579b38e7291c541fe.wav\n",
      "Copied: flat_new/ad58f5f2f830aa4697cde07cab90204e.wav\n",
      "Copied: flat_new/76794b05a8cf632ff3e15b31edd00df8.wav\n",
      "Copied: flat_new/93e7fc7efc60f568666cfcbe1e9b4180.wav\n",
      "Copied: flat_new/1c55e5826b8de3908b38fb87dc595aa9.wav\n",
      "Copied: flat_new/581ddc7863a0701b155398c1a3ac1d6c.wav\n",
      "Copied: flat_new/5ad465f260f55decd341982e830dedc9.wav\n",
      "Copied: flat_new/3999501edc9baad06ea1714fdfe23e6d.wav\n",
      "Copied: flat_new/4726e7f8e22fe6a87370dc954fb585de.wav\n",
      "Copied: flat_new/dc8fd2db4e0dfd08a27fd84e6909fb9e.wav\n",
      "Copied: flat_new/c48783a083cf164d2f151f9705c393fb.wav\n",
      "Copied: flat_new/f30cb3906756aaf67f80b1d296c89d8c.wav\n",
      "Copied: flat_new/5e31fce9f284f44c21c883915126e6c7.wav\n",
      "Copied: flat_new/7da7c2931936e68839fc0bef6e49974e.wav\n",
      "Copied: flat_new/23a126d2c73f60f0c67e7d2da1a195b3.wav\n",
      "Copied: flat_new/daa5ab0dc55cb7371e850c49dc370f69.wav\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import hashlib\n",
    "\n",
    "def hash_audio_file(file_path):\n",
    "    \"\"\"Generate a hash for the audio file based on its content.\"\"\"\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "    # Create a hash of the audio data\n",
    "    audio_hash = hashlib.md5(y.tobytes()).hexdigest()\n",
    "    return audio_hash\n",
    "\n",
    "def flatten_wav_files(source_dir, target_dir):\n",
    "    if not os.path.exists(target_dir):\n",
    "        os.makedirs(target_dir)\n",
    "\n",
    "    seen_files = set()\n",
    "\n",
    "    for root, _, files in os.walk(source_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.wav'):\n",
    "                file_path = os.path.join(root, file)\n",
    "\n",
    "                # Generate a hash for the audio file\n",
    "                try:\n",
    "                    audio_hash = hash_audio_file(file_path)\n",
    "                    new_file_name = f\"{audio_hash}.wav\"\n",
    "\n",
    "                    # Check for duplicates\n",
    "                    if new_file_name not in seen_files:\n",
    "                        seen_files.add(new_file_name)\n",
    "                        new_file_path = os.path.join(target_dir, new_file_name)\n",
    "\n",
    "                        # Load the audio file and save it to the target directory\n",
    "                        y, sr = librosa.load(file_path, sr=None)\n",
    "                        sf.write(new_file_path, y, sr)\n",
    "                        print(f\"Copied: {new_file_path}\")\n",
    "                    else:\n",
    "                        print(f\"Duplicate found: {file_path}, skipping.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "source_directory = 'data_copy'\n",
    "target_directory = 'flat_new'\n",
    "\n",
    "flatten_wav_files(source_directory, target_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c3b5282-9257-4758-9d21-4a8b6813d1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "def preprocess_audio(file_path, target_sample_rate=22050, target_length=661500):\n",
    "    # Load the audio file\n",
    "    waveform, sample_rate = torchaudio.load(file_path)\n",
    "\n",
    "    # Resample if the sample rate is different\n",
    "    if sample_rate != target_sample_rate:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=target_sample_rate)\n",
    "        waveform = resampler(waveform)\n",
    "\n",
    "    # Trim or pad to target length\n",
    "    if waveform.size(1) > target_length:\n",
    "        waveform = waveform[:, :target_length]\n",
    "    elif waveform.size(1) < target_length:\n",
    "        padding = target_length - waveform.size(1)\n",
    "        waveform = torch.nn.functional.pad(waveform, (0, padding))\n",
    "\n",
    "    return waveform\n",
    "\n",
    "# Apply to all audio files\n",
    "def load_and_preprocess_data(data_dir):\n",
    "    audio_files = []\n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.endswith(\".wav\"):\n",
    "            file_path = os.path.join(data_dir, filename)\n",
    "            audio_files.append(preprocess_audio(file_path))\n",
    "    return audio_files\n",
    "\n",
    "# Example usage\n",
    "data_dir = \"flat_new\"\n",
    "processed_audio = load_and_preprocess_data(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8a956e2-b865-470a-b69c-b64e24835c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, audio_data):\n",
    "        self.audio_data = audio_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Squeeze the waveform to remove unnecessary dimensions\n",
    "        return self.audio_data[idx].squeeze(0)  # This will convert [1, 661500] to [661500]\n",
    "\n",
    "\n",
    "# Create DataLoader\n",
    "audio_dataset = AudioDataset(processed_audio)\n",
    "audio_loader = DataLoader(audio_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f980b163-6377-4363-bfda-f76a9449c87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class AudioGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AudioGenerator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(100, 512),  # Input dimension is 100 for noise\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 661500),  # Output dimension for audio samples\n",
    "            nn.Tanh()  # Output normalized to [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "generator = AudioGenerator()\n",
    "optimizer = torch.optim.Adam(generator.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5257ec37-bd43-4fa7-bd82-ebd56626d63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Epoch: 0 complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Epoch: 1 complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Epoch: 2 complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Epoch: 3 complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Epoch: 4 complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Epoch: 5 complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Epoch: 6 complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Epoch: 7 complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Epoch: 8 complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Batch complete\n",
      "Epoch: 9 complete\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    for batch in audio_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Generate noise input\n",
    "        noise = torch.randn(batch.size(0), 100)  # Adjust according to your noise dimension\n",
    "        generated_audio = generator(noise)\n",
    "        \n",
    "        # Ensure both generated_audio and batch are in the shape [batch_size, 661500]\n",
    "        loss = criterion(generated_audio, batch)  # Both should now match\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('Batch complete')\n",
    "    print('Epoch:', epoch,'complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "755c21a7-ddcc-4374-8d72-53c759510079",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    noise = torch.randn(1, 100)  # Generate noise\n",
    "    generated_audio = generator(noise)\n",
    "\n",
    "# Optionally save the generated audio\n",
    "torchaudio.save(\"generated_audio_new2.wav\", generated_audio, 22050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "349e3d8b-9c55-4c48-8bb0-f224b06b5857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.0000\n",
      "Epoch [1/10], Loss: 3712.9907\n",
      "Epoch [1/10], Loss: 109.6233\n",
      "Epoch [1/10], Loss: 134.4192\n",
      "Epoch [1/10], Loss: 311.2141\n",
      "Epoch [1/10], Loss: 225.3912\n",
      "Epoch [1/10], Loss: 65.1125\n",
      "Epoch [2/10], Loss: 2.1817\n",
      "Epoch [2/10], Loss: 12.7934\n",
      "Epoch [2/10], Loss: 18.7875\n",
      "Epoch [2/10], Loss: 11.8786\n",
      "Epoch [2/10], Loss: 3.1221\n",
      "Epoch [2/10], Loss: 1.0223\n",
      "Epoch [2/10], Loss: 3.7862\n",
      "Epoch [3/10], Loss: 1.6029\n",
      "Epoch [3/10], Loss: 0.0210\n",
      "Epoch [3/10], Loss: 0.8483\n",
      "Epoch [3/10], Loss: 0.7811\n",
      "Epoch [3/10], Loss: 0.4956\n",
      "Epoch [3/10], Loss: 0.1749\n",
      "Epoch [3/10], Loss: 0.0100\n",
      "Epoch [4/10], Loss: 0.0519\n",
      "Epoch [4/10], Loss: 0.1993\n",
      "Epoch [4/10], Loss: 0.2640\n",
      "Epoch [4/10], Loss: 0.0857\n",
      "Epoch [4/10], Loss: 0.0902\n",
      "Epoch [4/10], Loss: 0.0361\n",
      "Epoch [4/10], Loss: 0.0054\n",
      "Epoch [5/10], Loss: 0.0406\n",
      "Epoch [5/10], Loss: 0.0418\n",
      "Epoch [5/10], Loss: 0.0214\n",
      "Epoch [5/10], Loss: 0.0041\n",
      "Epoch [5/10], Loss: 0.0013\n",
      "Epoch [5/10], Loss: 0.0114\n",
      "Epoch [5/10], Loss: 0.0159\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Define the audio dataset class\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, audio_data):\n",
    "        self.audio_data = audio_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Squeeze the waveform to remove unnecessary dimensions\n",
    "        return self.audio_data[idx].squeeze(0)  # This will convert [1, 661500] to [661500]\n",
    "\n",
    "# Define the CNN model\n",
    "class AudioCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AudioCNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=16, kernel_size=5, stride=2, padding=2),  # First Conv Layer\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16, 32, kernel_size=5, stride=2, padding=2),  # Second Conv Layer\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 64, kernel_size=5, stride=2, padding=2),  # Third Conv Layer\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 128, kernel_size=5, stride=2, padding=2),  # Fourth Conv Layer\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, 256, kernel_size=5, stride=2, padding=2),  # Fifth Conv Layer\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Using a test input to find the correct input size for the linear layer\n",
    "        self.test_input = torch.zeros(1, 1, 661500)  # [batch_size, channels, length]\n",
    "        self.flattened_size = self._get_flattened_size(self.test_input)\n",
    "        self.fc = nn.Linear(self.flattened_size, 1)  # Fully connected layer; adjust input size accordingly\n",
    "\n",
    "    def _get_flattened_size(self, input_tensor):\n",
    "        with torch.no_grad():  # No need to track gradients here\n",
    "            output = self.conv_layers(input_tensor)\n",
    "        return output.numel()  # Get the number of elements in the output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, 661500)  # Add the channel dimension\n",
    "        x = self.conv_layers(x)  # Pass through convolutional layers\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.fc(x)  # Pass through fully connected layer\n",
    "        return x\n",
    "\n",
    "# Create an instance of the model\n",
    "model = AudioCNN()\n",
    "\n",
    "# Create DataLoader with your processed audio data\n",
    "# processed_audio should be a tensor of shape [N, 1, 661500] where N is the number of audio samples\n",
    "audio_dataset = AudioDataset(processed_audio)\n",
    "audio_loader = DataLoader(audio_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Example training loop (simplified)\n",
    "for epoch in range(5):  # Number of epochs\n",
    "    for batch in audio_loader:\n",
    "        optimizer.zero_grad()  # Zero gradients\n",
    "        output = model(batch)  # Forward pass\n",
    "        \n",
    "        # Define target variable appropriately here\n",
    "        # For example, it could be the same shape as output if doing regression on audio\n",
    "        target = torch.zeros(batch.size(0), 1)  # Placeholder for target, replace with actual targets\n",
    "        \n",
    "        loss = criterion(output, target)  # Calculate loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "        \n",
    "        print(f\"Epoch [{epoch + 1}/10], Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c0d45f48-2690-4e46-a41b-148201b55a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate noise\n",
    "with torch.no_grad():\n",
    "    noise = torch.randn(1, 1, 100)  # Shape: (batch_size, channels, noise_dim)\n",
    "    generated_audio = generator(noise)\n",
    "\n",
    "# Ensure the output is in the correct shape for torchaudio.save\n",
    "generated_audio = generated_audio.view(1, -1)  # Flatten the tensor to (1, audio_length)\n",
    "\n",
    "# Save the generated audio to a file\n",
    "torchaudio.save(\"generated_audio_new3.wav\", generated_audio, 22050) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a94f4f-9081-4826-9a64-27af831cad36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
