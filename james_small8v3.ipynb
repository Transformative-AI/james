{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5b2671b-ff18-4e25-ae61-4598caeb2142",
   "metadata": {},
   "source": [
    "## Jump to the next section for the updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d8e7142-6f2e-42d6-8682-14584ec426cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import requests\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from nba_api.stats.endpoints import leaguegamefinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "731c9187-a192-41f6-bfdb-55dd78633c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0041900406', '0041900405', '0041900404', ..., '0011900004',\n",
       "       '0011900002', '0011900001'], dtype=object)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_season_games(season_year='2019-20'):\n",
    "    gamefinder = leaguegamefinder.LeagueGameFinder(season_nullable=season_year)\n",
    "    games_df = gamefinder.get_data_frames()[0]\n",
    "    return games_df['GAME_ID'].unique()\n",
    "\n",
    "game_ids = get_season_games('2019-20')\n",
    "game_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f8b2f131-d441-44ca-b929-1cdedb3f48ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2021900519' '0021900878' '0021900882' '0021900881' '0021900880']\n"
     ]
    }
   ],
   "source": [
    "subsetGMs=game_ids[350:360:2]\n",
    "print(subsetGMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a67dbc9e-6148-4992-a706-1326ca27299b",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'Host': 'stats.nba.com',\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:72.0) Gecko/20100101 Firefox/72.0',\n",
    "    'Accept': 'application/json, text/plain, */*',\n",
    "    'Accept-Language': 'en-US,en;q=0.5',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'x-nba-stats-origin': 'stats',\n",
    "    'x-nba-stats-token': 'true',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Referer': 'https://stats.nba.com/',\n",
    "    'Pragma': 'no-cache',\n",
    "    'Cache-Control': 'no-cache'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3e069ad2-a1cb-43ed-9a50-bf11d3273d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.0% of 2021900519 complete\n",
      "50.0% of 2021900519 complete\n",
      "75.0% of 2021900519 complete\n",
      "100.0% of 2021900519 complete\n",
      "25.0% of 0021900878 complete\n",
      "50.0% of 0021900878 complete\n",
      "75.0% of 0021900878 complete\n",
      "100.0% of 0021900878 complete\n",
      "25.0% of 0021900882 complete\n",
      "50.0% of 0021900882 complete\n",
      "75.0% of 0021900882 complete\n",
      "100.0% of 0021900882 complete\n",
      "25.0% of 0021900881 complete\n",
      "50.0% of 0021900881 complete\n",
      "75.0% of 0021900881 complete\n",
      "100.0% of 0021900881 complete\n",
      "25.0% of 0021900880 complete\n",
      "50.0% of 0021900880 complete\n",
      "75.0% of 0021900880 complete\n",
      "100.0% of 0021900880 complete\n"
     ]
    }
   ],
   "source": [
    "successful_video_events=[]\n",
    "for gid in subsetGMs:\n",
    "    for i in range(200):\n",
    "        time.sleep(.2)\n",
    "        if (i+1)%50 == 0:\n",
    "            print(str((i+1)/2)+'% of', gid ,'complete')\n",
    "        video_count=0\n",
    "        event_id = i\n",
    "        game_id = gid\n",
    "        url = 'https://stats.nba.com/stats/videoeventsasset?GameEventID={}&GameID={}'.format(\n",
    "            event_id, game_id)\n",
    "        try:\n",
    "            r = requests.get(url, headers=headers)\n",
    "            json = r.json()\n",
    "            video_urls = json['resultSets']['Meta'].get('videoUrls', [])\n",
    "            playlist = json['resultSets'].get('playlist', [])\n",
    "            \n",
    "            if video_urls and playlist:  # Ensure there are items before accessing\n",
    "                video_event = {'video': video_urls[0]['lurl'], 'desc': playlist[0]['dsc']}\n",
    "                successful_video_events.append(video_event)\n",
    "    \n",
    "        except KeyError as e:\n",
    "            print(f\"KeyError encountered for event ID {event_id}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred for event ID {event_id}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d3888d0c-50ae-4992-ae6e-194c708d6e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "sve=successful_video_events.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "113e3011-96ca-4011-8600-222c9913e471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720 720\n"
     ]
    }
   ],
   "source": [
    "sve_backup=sve.copy()\n",
    "print(len(sve),len(sve_backup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7ccab988-c6c4-45fc-96f7-40e1eb22790d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before pruning: 720\n",
      "makes: 95 \n",
      "miss: 172\n"
     ]
    }
   ],
   "source": [
    "print('before pruning:',len(sve))\n",
    "\n",
    "make=[]\n",
    "miss=[]\n",
    "\n",
    "for i in sve:\n",
    "    # if not i['video']:\n",
    "    #     sve.remove(i)\n",
    "    # else:\n",
    "    if 'MISS' in i['desc']:\n",
    "        miss.append(i)\n",
    "    elif 'Shot' in i['desc']:\n",
    "        make.append(i)\n",
    "        # else:\n",
    "        #     sve.remove(i)\n",
    "# print('after pruning:',len(sve),'\\n makes:',len(make),'\\n miss:',len(miss))\n",
    "print('makes:',len(make),'\\nmiss:',len(miss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beff75f3-0497-4907-8722-ac1826ac3559",
   "metadata": {},
   "source": [
    "## This is where new edits and code are added since v2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "755b045c-4c00-402b-963a-0de8d00a7918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for m in make:\n",
    "    if 'Shot' in m['desc']:\n",
    "        count+=1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "47cf2549-c3a8-44e4-942e-b8e363444367",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls=[]\n",
    "#now that we have our urls for our training data we can retrieve the videos\n",
    "for u in make+miss:\n",
    "    urls.append(i['video'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f9083520-6511-4e74-ba69-368dab4d4bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "267"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "135c7fbe-c263-4732-86a2-3590cfa10feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_video(url, filename):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        # Specify subfolder\n",
    "        folder_name = \"clips_test\"\n",
    "        os.makedirs(folder_name, exist_ok=True)  # Create folder if it doesn't exist\n",
    "        full_path = os.path.join(os.getcwd(),folder_name, filename)\n",
    "        with open(full_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        return full_path\n",
    "    else:\n",
    "        print(f\"Failed to download video: {response.status_code}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "2e5562a9-6d24-4f63-b05c-a73ee14436cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, url in enumerate(urls):\n",
    "    filename= f\"test_nba_clip{idx + 1}.mp4\"  # Create a filename like test_nba_clip1.mp4\n",
    "    download_video(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff22bf07-6348-4bd6-8101-aca63409588a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_video(video_path, frame_rate=1, resize_dim=(224, 224)):\n",
    "    # Create a list to hold the processed frames\n",
    "    frames = []\n",
    "\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error opening video file: {video_path}\")\n",
    "        return []\n",
    "\n",
    "    frame_count = 0\n",
    "    while True:\n",
    "        # Read a frame from the video\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Extract frames at the specified frame rate\n",
    "        if frame_count % frame_rate == 0:\n",
    "            # Resize the frame\n",
    "            frame = cv2.resize(frame, resize_dim)\n",
    "            \n",
    "            # Normalize the pixel values to [0, 1]\n",
    "            frame = frame / 255.0\n",
    "            \n",
    "            # Append the processed frame to the list\n",
    "            frames.append(frame)\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    # Release the video capture object\n",
    "    cap.release()\n",
    "    \n",
    "    # Convert list of frames to a NumPy array\n",
    "    return np.array(frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa291c14-5469-493c-8b70-339f6efd1e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267\n"
     ]
    }
   ],
   "source": [
    "folder_name = \"clips_test\"\n",
    "video_files = [f for f in os.listdir(folder_name) if f.endswith('.mp4')]\n",
    "\n",
    "all_processed_frames = []  # List to hold frames from all videos\n",
    "\n",
    "for video_file in video_files:\n",
    "    video_path = os.path.join(folder_name, video_file)\n",
    "    processed_frames = preprocess_video(video_path)\n",
    "    \n",
    "    # print(f\"Processed {video_file}: Number of frames extracted and processed: {len(processed_frames)}\")\n",
    "    \n",
    "    # Append the processed frames to the all_processed_frames list\n",
    "    all_processed_frames.append(processed_frames)\n",
    "print(len(all_processed_frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de4f6338-c5f5-4b06-b0cd-6968d0ce8f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_train=[]\n",
    "miss_train=[]\n",
    "make_test=[]\n",
    "miss_test=[]\n",
    "data_dict={1: [],0:[]}\n",
    "\n",
    "for i,pf in enumerate(all_processed_frames):\n",
    "    if i < 95: #len(make) if my computer force a restart\n",
    "        data_dict[1]+=[pf]\n",
    "    else:\n",
    "        data_dict[0]+=[pf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f4c287b-f7fe-4cb4-b060-62868b60bc9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_dict[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f293110-227c-423a-98d6-8996c76d8c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_train=data_dict[1][:76]\n",
    "miss_train=data_dict[0][:139]\n",
    "make_test=data_dict[1][76:]\n",
    "miss_test=data_dict[0][139:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6d28a3e-0100-4b19-8d3d-d58293927785",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5975a23a-dbe8-420c-8d02-376dcdfec4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df35e40-e263-45ec-97ac-04b7c9b391b3",
   "metadata": {},
   "source": [
    "## This is where things start to go bad. My kernel dies trying to create a dataframe from this data. Yaarghh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "933afaea-15e7-4fed-b17e-62345cf4084c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e9a2f22-aaed-4fe5-aaa1-1edf88e1a8cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_train\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mDataFrame(make_train, miss_train)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# df_train=pd.DataFrame(make_train, miss_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6b9ff645-a7cb-4fb0-a475-60041eaf88aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215\n"
     ]
    }
   ],
   "source": [
    "print(len(make_train + miss_train))  # Check how many samples you have\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d027e0-c76f-4c91-a81c-ef747cff8bae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a92e1a3d-0593-4664-bc07-f712456c5832",
   "metadata": {},
   "source": [
    "### On an another attempt, my kernel ends up needing to restart again. Sad face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54923a8c-610c-46c9-aa93-fba8f69fb79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(make_train + miss_train).reshape(-1, 1)  # Reshape to (num_samples, 1)\n",
    "y_train = np.array([1]*76 + [0]*139)  # Labels for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681361b7-f665-41fd-be10-b91ce61eeadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#My kernel died again... I need to reconsider my data. Right now the nested lists/arrays are too big to handle especially when the clips are 12 seconds long and the files take time and space to preprocess.\n",
    "#This is what I would do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b419795f-0181-4a0f-9124-6cbc4ca84e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_shape=(1,)))  # One feature per sample\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))  # Two output classes (made or missed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f92679-b5b9-43bb-bffb-ee7eec92576a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ed2378-7395-4560-bffa-a02f27debfe8",
   "metadata": {},
   "source": [
    "### I should try downsampling my data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da8553a-e533-4a69-b721-5babec3c9b2a",
   "metadata": {},
   "source": [
    "### I can try preprocessing in a different way using something like this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e654bd2a-797f-4afc-8dcc-b90369bfd2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c7/tjnc1nv13vl7684g7d2d8thm0000gn/T/ipykernel_60432/3331083074.py:2: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  model = MobileNetV2(weights='imagenet', include_top=False, pooling='avg')\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained model\n",
    "model = MobileNetV2(weights='imagenet', include_top=False, pooling='avg')\n",
    "\n",
    "def extract_features_from_frames(frame_paths):\n",
    "    features = []\n",
    "    \n",
    "    for frame_path in frame_paths:\n",
    "        img = image.load_img(frame_path, target_size=(224, 224))  # Resize to fit the model\n",
    "        img_array = image.img_to_array(img)\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        img_array = preprocess_input(img_array)  # Preprocess for MobileNetV2\n",
    "        \n",
    "        # Extract features\n",
    "        feature = model.predict(img_array)\n",
    "        features.append(feature.flatten())\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "# Example usage\n",
    "frame_folder = \"key_frames\"  # Folder containing extracted frames\n",
    "frame_paths = [os.path.join(frame_folder, f) for f in os.listdir(frame_folder) if f.endswith('.jpg')]\n",
    "\n",
    "# Extract features from key frames\n",
    "features = extract_features_from_frames(frame_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1013ca1a-b4db-4ce8-9393-ee5d30816a1d",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Define the model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential()\n\u001b[0;32m----> 3\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[38;5;241m128\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m(features\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],)))\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dropout(\u001b[38;5;241m0.5\u001b[39m))\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[38;5;241m64\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_shape=(features.shape[1],)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))  # num_classes = number of event types\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit([1]*76+[0]*139, make_train+miss_train, epochs=10, batch_size=32, validation_data=([1]*19+[0]*37, make_test+miss_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf9c3d9-4db1-422a-8432-a278f53c59d0",
   "metadata": {},
   "source": [
    "## Or I can follow another method like this: https://github.com/hkair/Basketball-Action-Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42965223-1888-4612-a60a-0e80c3a8ef7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
